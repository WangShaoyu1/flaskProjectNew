import os
import time
import json
from pathlib import Path
from openai import OpenAI
from dotenv import load_dotenv
from utils import util, log

# è®°å½•å¼€å§‹æ—¶é—´
start_time = time.time()
first_char_time = None
full_response = []

# åŠ è½½.envæ–‡ä»¶ä¸­çš„ç¯å¢ƒå˜é‡
load_dotenv()


# åŠ è½½tools.jsonæ–‡ä»¶
def load_tools_config():
    tools_path = Path(__file__).parent / "tools.json"
    try:
        with open(tools_path, 'r', encoding='utf-8') as f:
            return json.load(f)
    except FileNotFoundError:
        print(f"âš ï¸ è­¦å‘Šï¼šæœªæ‰¾åˆ°å·¥å…·é…ç½®æ–‡ä»¶ {tools_path}")
        return None
    except json.JSONDecodeError:
        print(f"âš ï¸ è­¦å‘Šï¼šå·¥å…·é…ç½®æ–‡ä»¶ {tools_path} æ ¼å¼é”™è¯¯")
        return None


# è·å–å·¥å…·é…ç½®
tools_config = []

client = OpenAI(
    api_key=os.getenv("QWEN_DASHSCOPE_API_KEY"),
    base_url="https://dashscope.aliyuncs.com/compatible-mode/v1",

)

# æ„é€ è¯·æ±‚å‚æ•°
request_params = {
    "model": "qwen-plus",
    "messages": [
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": "æ¸¯è‚¡é€šé˜¿é‡Œå·´å·´ä»Šå¤©è‚¡ä»·æ€ä¹ˆæ ·ï¼Ÿ"},
    ],
    "stream": True,
    "max_tokens": 500,
    "extra_body": {"enable_thinking": False, "enable_search": True},
}

# å¦‚æœæœ‰å·¥å…·é…ç½®ï¼Œæ·»åŠ åˆ°è¯·æ±‚å‚æ•°ä¸­
# å¦‚æœå­˜åœ¨å·¥å…·é…ç½®åˆ™æ·»åŠ 
if tools_config:
    request_params["tools"] = tools_config.get("tools", [])
    request_params["tool_choice"] = tools_config.get("tool_choice", "auto")
    # print("âœ… å·²åŠ è½½å·¥å…·é…ç½®:", json.dumps(tools_config, indent=2, ensure_ascii=False))

# å‘èµ·æµå¼è¯·æ±‚
stream = client.chat.completions.create(**request_params)

print("\næµå¼å“åº”å¼€å§‹ï¼š")
for chunk in stream:
    # è·å–å½“å‰chunkå†…å®¹
    content = chunk.choices[0].delta.content or ""

    # è®°å½•é¦–å­—ç¬¦åˆ°è¾¾æ—¶é—´(TTFB)
    if content and first_char_time is None:
        first_char_time = time.time() - start_time
        print(f"\n[é¦–å­—ç¬¦åˆ°è¾¾] {first_char_time * 1000:.2f}ms")

    # æ”¶é›†å®Œæ•´å“åº”
    full_response.append(content)
    print(content, end="", flush=True)  # å®æ—¶è¾“å‡º

    # æ£€æŸ¥æ˜¯å¦åŒ…å«å·¥å…·è°ƒç”¨
    if chunk.choices[0].delta.tool_calls:
        print("\nğŸ”§ æ£€æµ‹åˆ°å·¥å…·è°ƒç”¨:", chunk.choices[0].delta.tool_calls)

# è®¡ç®—å®Œæ•´å“åº”æ—¶é—´
end_time = time.time()
total_time = (end_time - start_time) * 1000  # æ¯«ç§’

# æ€§èƒ½æŠ¥å‘Š
# æ€§èƒ½æŠ¥å‘Š
performance_report = f"""
{'=' * 40}
é¦–å­—ç¬¦æ—¶é—´(TTFB): {first_char_time * 1000:.2f}ms
å®Œæ•´å“åº”æ—¶é—´: {total_time:.2f}ms
æ€»å­—ç¬¦æ•°: {len(''.join(full_response))}
ç”Ÿæˆé€Ÿåº¦: {len(''.join(full_response)) / total_time * 1000:.2f} char/s
å®Œæ•´å“åº”: {''.join(full_response)}
{'=' * 40}
"""
util.write_to_file("performance.log", performance_report)
