#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¢å¼ºç‰ˆRasaè®­ç»ƒæ•°æ®ç”Ÿæˆå™¨
æ”¯æŒå®ä½“å’Œæ§½ä½ä¿¡æ¯
"""

import json
import yaml
import os
from datetime import datetime

def load_enhanced_intents(json_file):
    """åŠ è½½å¢å¼ºç‰ˆæ„å›¾æ•°æ®"""
    try:
        with open(json_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
        return data
    except Exception as e:
        print(f"åŠ è½½JSONæ–‡ä»¶å¤±è´¥: {e}")
        return None

def generate_nlu_data(intents_data):
    """ç”ŸæˆNLUè®­ç»ƒæ•°æ®"""
    nlu_data = {
        'version': '3.1',
        'nlu': []
    }
    
    for intent in intents_data['intents']:
        intent_name = intent['intent_name']
        
        # æ„å»ºè®­ç»ƒæ ·ä¾‹
        examples = []
        for utterance in intent['utterances']:
            if utterance['entities']:
                # æœ‰å®ä½“æ ‡æ³¨çš„æ ·ä¾‹
                text = utterance['text']
                entities_text = ""
                
                # æŒ‰ä½ç½®æ’åºå®ä½“
                sorted_entities = sorted(utterance['entities'], key=lambda x: x['start'])
                
                current_pos = 0
                for entity in sorted_entities:
                    # æ·»åŠ å®ä½“å‰çš„æ–‡æœ¬
                    if entity['start'] > current_pos:
                        entities_text += text[current_pos:entity['start']]
                    
                    # æ·»åŠ å®ä½“æ ‡æ³¨
                    entity_text = text[entity['start']:entity['end']]
                    entities_text += f"[{entity_text}]({entity['entity']})"
                    
                    current_pos = entity['end']
                
                # æ·»åŠ å‰©ä½™æ–‡æœ¬
                if current_pos < len(text):
                    entities_text += text[current_pos:]
                
                examples.append(f"- {entities_text}")
            else:
                # æ²¡æœ‰å®ä½“çš„æ ·ä¾‹
                examples.append(f"- {utterance['text']}")
        
        if examples:
            nlu_data['nlu'].append({
                'intent': intent_name,
                'examples': '\n'.join(examples)
            })
    
    return nlu_data

def generate_domain_data(intents_data):
    """ç”ŸæˆDomainæ•°æ®"""
    domain_data = {
        'version': '3.1',
        'intents': [],
        'entities': [],
        'slots': {},
        'responses': {}
    }
    
    # æ·»åŠ æ„å›¾
    for intent in intents_data['intents']:
        domain_data['intents'].append(intent['intent_name'])
    
    # æ·»åŠ å®ä½“
    for entity in intents_data['entities']:
        domain_data['entities'].append(entity['entity'])
    
    # æ·»åŠ æ§½ä½
    for slot_name, slot_info in intents_data['slots'].items():
        domain_data['slots'][slot_name] = {
            'type': 'categorical',
            'values': slot_info['values'],
            'mappings': [
                {
                    'type': 'from_entity',
                    'entity': slot_name
                }
            ]
        }
    
    # æ·»åŠ å“åº”
    for intent in intents_data['intents']:
        intent_name = intent['intent_name']
        response_key = f"utter_{intent_name}"
        
        responses = []
        for response in intent['responses']:
            if response['type'] == 'success':
                responses.append({'text': response['text']})
        
        if not responses:
            # å¦‚æœæ²¡æœ‰æˆåŠŸå“åº”ï¼Œæ·»åŠ é»˜è®¤å“åº”
            responses.append({'text': f"å·²æ‰§è¡Œ{intent.get('description', intent_name)}"})
        
        domain_data['responses'][response_key] = responses
    
    return domain_data

def generate_rules_data(intents_data):
    """ç”ŸæˆRulesæ•°æ®ï¼ˆå•è½®å¯¹è¯è§„åˆ™ï¼‰"""
    rules_data = {
        'version': '3.1',
        'rules': []
    }
    
    for intent in intents_data['intents']:
        intent_name = intent['intent_name']
        rule_name = f"Rule for {intent_name}"
        
        rule = {
            'rule': rule_name,
            'steps': [
                {'intent': intent_name},
                {'action': f"utter_{intent_name}"}
            ]
        }
        
        rules_data['rules'].append(rule)
    
    return rules_data

def generate_config_data():
    """ç”Ÿæˆä¼˜åŒ–çš„é…ç½®æ•°æ®ï¼ˆä¸“æ³¨äºå•è½®å¯¹è¯ï¼Œä¸ä½¿ç”¨storiesï¼‰"""
    config_data = {
        'recipe': 'default.v1',
        'language': 'zh',
        'pipeline': [
            {'name': 'JiebaTokenizer'},
            {'name': 'RegexFeaturizer'},
            {'name': 'LexicalSyntacticFeaturizer'},
            {
                'name': 'CountVectorsFeaturizer',
                'analyzer': 'char_wb',
                'min_ngram': 1,
                'max_ngram': 4
            },
            {
                'name': 'CountVectorsFeaturizer',
                'analyzer': 'word',
                'min_ngram': 1,
                'max_ngram': 2
            },
            {
                'name': 'DIETClassifier',
                'epochs': 200,
                'entity_recognition': True,
                'intent_classification': True,
                'use_masked_language_model': True,
                'transformer_size': 256,
                'number_of_transformer_layers': 2,
                'number_of_attention_heads': 4,
                'batch_size': 64,
                'evaluate_every_number_of_epochs': 20,
                'evaluate_on_number_of_examples': 0,
                'tensorboard_log_directory': './tensorboard',
                'tensorboard_log_level': 'epoch'
            },
            {
                'name': 'EntitySynonymMapper'
            },
            {
                'name': 'ResponseSelector',
                'epochs': 100,
                'retrieval_intent': None
            }
        ],
        'policies': [
            {
                'name': 'RulePolicy',
                'core_fallback_threshold': 0.3,
                'core_fallback_action_name': 'action_default_fallback',
                'enable_fallback_prediction': True
            },
            {
                'name': 'UnexpecTEDIntentPolicy',
                'max_history': 5,
                'epochs': 100,
                'batch_size': 32
            },
            {
                'name': 'TEDPolicy',
                'max_history': 5,
                'epochs': 100,
                'batch_size': 32,
                'evaluate_every_number_of_epochs': 20,
                'evaluate_on_number_of_examples': 0
            }
        ]
    }
    
    return config_data

def save_yaml_file(data, file_path):
    """ä¿å­˜YAMLæ–‡ä»¶"""
    try:
        with open(file_path, 'w', encoding='utf-8') as f:
            yaml.dump(data, f, default_flow_style=False, allow_unicode=True, sort_keys=False)
        print(f"âœ… ç”Ÿæˆæ–‡ä»¶: {file_path}")
        return True
    except Exception as e:
        print(f"âŒ ç”Ÿæˆæ–‡ä»¶å¤±è´¥ {file_path}: {e}")
        return False

def main():
    """ä¸»å‡½æ•°"""
    print("=" * 60)
    print("å¢å¼ºç‰ˆRasaè®­ç»ƒæ•°æ®ç”Ÿæˆå™¨")
    print("æ”¯æŒå®ä½“å’Œæ§½ä½ä¿¡æ¯")
    print("=" * 60)
    
    # è¾“å…¥è¾“å‡ºæ–‡ä»¶è·¯å¾„
    input_file = "intents_enhanced.json"
    output_dir = "../rasa/data"
    
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    os.makedirs(output_dir, exist_ok=True)
    
    # åŠ è½½å¢å¼ºç‰ˆæ„å›¾æ•°æ®
    print(f"ğŸ“– åŠ è½½æ•°æ®æ–‡ä»¶: {input_file}")
    intents_data = load_enhanced_intents(input_file)
    
    if not intents_data:
        print("âŒ æ— æ³•åŠ è½½æ•°æ®æ–‡ä»¶ï¼Œé€€å‡º")
        return
    
    # æ‰“å°æ•°æ®ç»Ÿè®¡
    print(f"\nğŸ“Š æ•°æ®ç»Ÿè®¡:")
    print(f"   æ„å›¾æ•°é‡: {intents_data['metadata']['total_intents']}")
    print(f"   å®ä½“ç±»å‹: {intents_data['metadata']['total_entities']}")
    print(f"   æ§½ä½æ•°é‡: {intents_data['metadata']['total_slots']}")
    print(f"   è®­ç»ƒè¯­å¥: {intents_data['metadata']['total_utterances']}")
    
    print(f"\nğŸ—ï¸ ç”ŸæˆRasaè®­ç»ƒæ–‡ä»¶...")
    
    # ç”Ÿæˆå„ç§æ•°æ®æ–‡ä»¶
    success_count = 0
    
    # 1. ç”ŸæˆNLUæ•°æ®
    print(f"\n1. ç”ŸæˆNLUæ•°æ®...")
    nlu_data = generate_nlu_data(intents_data)
    if save_yaml_file(nlu_data, os.path.join(output_dir, "nlu.yml")):
        success_count += 1
        print(f"   åŒ…å« {len(nlu_data['nlu'])} ä¸ªæ„å›¾")
    
    # 2. ç”ŸæˆDomainæ•°æ®
    print(f"\n2. ç”ŸæˆDomainæ•°æ®...")
    domain_data = generate_domain_data(intents_data)
    if save_yaml_file(domain_data, os.path.join(output_dir, "domain.yml")):
        success_count += 1
        print(f"   åŒ…å« {len(domain_data['intents'])} ä¸ªæ„å›¾")
        print(f"   åŒ…å« {len(domain_data['entities'])} ä¸ªå®ä½“ç±»å‹")
        print(f"   åŒ…å« {len(domain_data['slots'])} ä¸ªæ§½ä½")
        print(f"   åŒ…å« {len(domain_data['responses'])} ä¸ªå“åº”æ¨¡æ¿")
    
    # 3. ç”ŸæˆRulesæ•°æ®ï¼ˆå•è½®å¯¹è¯ï¼‰
    print(f"\n3. ç”ŸæˆRulesæ•°æ®ï¼ˆå•è½®å¯¹è¯ï¼‰...")
    rules_data = generate_rules_data(intents_data)
    if save_yaml_file(rules_data, os.path.join(output_dir, "rules.yml")):
        success_count += 1
        print(f"   åŒ…å« {len(rules_data['rules'])} æ¡è§„åˆ™")
    
    # 4. ç”Ÿæˆé…ç½®æ–‡ä»¶
    print(f"\n4. ç”Ÿæˆé…ç½®æ–‡ä»¶...")
    config_data = generate_config_data()
    if save_yaml_file(config_data, "../rasa/config.yml"):
        success_count += 1
        print(f"   ä¼˜åŒ–å•è½®å¯¹è¯é…ç½®")
    
    # åˆ é™¤stories.ymlæ–‡ä»¶ï¼ˆå› ä¸ºæˆ‘ä»¬ä¸“æ³¨äºå•è½®å¯¹è¯ï¼‰
    stories_file = os.path.join(output_dir, "stories.yml")
    if os.path.exists(stories_file):
        try:
            os.remove(stories_file)
            print(f"ğŸ—‘ï¸ åˆ é™¤stories.ymlæ–‡ä»¶ï¼ˆä¸“æ³¨å•è½®å¯¹è¯ï¼‰")
        except Exception as e:
            print(f"âš ï¸ åˆ é™¤stories.ymlå¤±è´¥: {e}")
    
    print(f"\n" + "=" * 60)
    print(f"ğŸ‰ ç”Ÿæˆå®Œæˆï¼æˆåŠŸç”Ÿæˆ {success_count}/4 ä¸ªæ–‡ä»¶")
    print(f"ğŸ“ è¾“å‡ºç›®å½•: {output_dir}")
    print(f"âš¡ é…ç½®ä¸“æ³¨äºå•è½®å¯¹è¯ï¼Œæ— éœ€stories.yml")
    print(f"ğŸš€ ç°åœ¨å¯ä»¥è¿è¡Œ: cd ../rasa && rasa train --force")
    print("=" * 60)
    
    # æ‰“å°å®ä½“å’Œæ§½ä½ä¿¡æ¯
    print(f"\nğŸ“‹ å®ä½“å’Œæ§½ä½è¯¦æƒ…:")
    for entity in intents_data['entities']:
        entity_name = entity['entity']
        entity_count = len(entity['values'])
        print(f"   ğŸ·ï¸ {entity_name}: {entity_count} ä¸ªå€¼")
        
        # æ˜¾ç¤ºå‰3ä¸ªå€¼ä½œä¸ºç¤ºä¾‹
        for i, value in enumerate(entity['values'][:3]):
            synonyms_info = f" (åˆ«å: {', '.join(value['synonyms'][:3])})" if value['synonyms'] else ""
            print(f"      â€¢ {value['value']}{synonyms_info}")
        
        if entity_count > 3:
            print(f"      ... è¿˜æœ‰ {entity_count - 3} ä¸ªå€¼")
    
    print(f"\nğŸ¯ æ„å›¾ä¸æ§½ä½å…³è”:")
    for intent in intents_data['intents']:
        if intent['entities']:
            print(f"   {intent['intent_name']}: {', '.join(intent['entities'])}")

if __name__ == "__main__":
    main() 