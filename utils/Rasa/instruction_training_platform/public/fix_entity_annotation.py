#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä¿®å¤å®ä½“æ ‡æ³¨é—®é¢˜çš„è„šæœ¬
å°† [å ä½ç¬¦](å ä½ç¬¦) æ ¼å¼ä¿®å¤ä¸º [å…·ä½“å€¼](å®ä½“ç±»å‹) æ ¼å¼
"""

import json
import yaml
import os
import pandas as pd
import re
from itertools import cycle

def load_slot_mapping():
    """åŠ è½½æ§½ä½æ˜ å°„æ•°æ®"""
    slot_file = "403/slot_403_1750302057.xlsx"
    
    try:
        df = pd.read_excel(slot_file, engine='openpyxl')
        slot_mapping = {}
        
        current_slot = None
        for _, row in df.iterrows():
            if pd.notna(row['è¯æ§½åç§°']):
                current_slot = row['è¯æ§½åç§°']
                slot_mapping[current_slot] = []
            
            if current_slot and pd.notna(row['å®ä½“æ ‡å‡†å']):
                entity_value = str(row['å®ä½“æ ‡å‡†å'])
                
                # æ·»åŠ ä¸»è¦å€¼
                slot_mapping[current_slot].append(entity_value)
                
                # æ·»åŠ åˆ«å
                if pd.notna(row['å®ä½“åˆ«å']):
                    aliases = str(row['å®ä½“åˆ«å']).split('==')
                    for alias in aliases:
                        alias = alias.strip()
                        if alias:
                            slot_mapping[current_slot].append(alias)
        
        print(f"âœ… åŠ è½½æ§½ä½æ˜ å°„æˆåŠŸï¼Œå…± {len(slot_mapping)} ä¸ªæ§½ä½")
        for slot, values in slot_mapping.items():
            print(f"   {slot}: {len(values)} ä¸ªå€¼")
        
        return slot_mapping
        
    except Exception as e:
        print(f"âŒ åŠ è½½æ§½ä½æ˜ å°„å¤±è´¥: {e}")
        return {}

def expand_entity_placeholder(text, entity_type, slot_mapping, max_examples=10):
    """
    å±•å¼€å®ä½“å ä½ç¬¦ä¸ºå…·ä½“çš„è®­ç»ƒæ ·ä¾‹
    
    Args:
        text: åŒ…å«å ä½ç¬¦çš„æ–‡æœ¬ï¼Œå¦‚ "åˆ‡æ¢[ç¬¬N]ä¸ªè§’è‰²"
        entity_type: å®ä½“ç±»å‹ï¼Œå¦‚ "ç¬¬N"
        slot_mapping: æ§½ä½æ˜ å°„å­—å…¸
        max_examples: æ¯ä¸ªå ä½ç¬¦æœ€å¤šç”Ÿæˆçš„æ ·ä¾‹æ•°
    
    Returns:
        List[Dict]: å±•å¼€åçš„è®­ç»ƒæ ·ä¾‹åˆ—è¡¨
    """
    
    if entity_type not in slot_mapping:
        # å¦‚æœæ‰¾ä¸åˆ°æ˜ å°„ï¼Œè¿”å›åŸæ–‡æœ¬ï¼ˆç§»é™¤æ–¹æ‹¬å·ï¼‰
        clean_text = re.sub(r'\[([^\]]+)\]', r'\1', text)
        return [{
            "text": clean_text,
            "entities": []
        }]
    
    # è·å–è¯¥å®ä½“ç±»å‹çš„æ‰€æœ‰å¯èƒ½å€¼
    entity_values = slot_mapping[entity_type]
    
    # é™åˆ¶æ ·ä¾‹æ•°é‡ï¼Œé¿å…è¿‡å¤š
    selected_values = entity_values[:max_examples]
    
    examples = []
    
    for entity_value in selected_values:
        # æ›¿æ¢å ä½ç¬¦ä¸ºå…·ä½“å€¼
        expanded_text = text.replace(f"[{entity_type}]", entity_value)
        
        # æ‰¾åˆ°å®ä½“åœ¨æ–‡æœ¬ä¸­çš„ä½ç½®
        start_pos = expanded_text.find(entity_value)
        if start_pos == -1:
            continue
            
        end_pos = start_pos + len(entity_value)
        
        example = {
            "text": expanded_text,
            "entities": [{
                "start": start_pos,
                "end": end_pos,
                "value": entity_value,
                "entity": entity_type
            }]
        }
        
        examples.append(example)
    
    return examples

def fix_training_data():
    """ä¿®å¤è®­ç»ƒæ•°æ®ä¸­çš„å®ä½“æ ‡æ³¨é—®é¢˜"""
    
    print("=" * 60)
    print("ä¿®å¤å®ä½“æ ‡æ³¨é—®é¢˜")
    print("=" * 60)
    
    # åŠ è½½æ§½ä½æ˜ å°„
    slot_mapping = load_slot_mapping()
    
    if not slot_mapping:
        print("âŒ æ— æ³•åŠ è½½æ§½ä½æ˜ å°„ï¼Œé€€å‡º")
        return
    
    # åŠ è½½åŸå§‹æ„å›¾æ•°æ®
    intents_file = "intents.json"
    try:
        with open(intents_file, 'r', encoding='utf-8') as f:
            intents_data = json.load(f)
        print(f"âœ… åŠ è½½åŸå§‹æ•°æ®: {len(intents_data['intents'])} ä¸ªæ„å›¾")
    except Exception as e:
        print(f"âŒ æ— æ³•åŠ è½½åŸå§‹æ•°æ®: {e}")
        return
    
    # ä¿®å¤åçš„æ•°æ®
    fixed_intents = []
    total_original_utterances = 0
    total_fixed_utterances = 0
    
    for intent in intents_data['intents']:
        intent_name = intent['intent_name']
        original_utterances = intent.get('utterances', [])
        total_original_utterances += len(original_utterances)
        
        fixed_utterances = []
        used_entities = set()
        
        for utterance_text in original_utterances:
            # æ£€æŸ¥æ˜¯å¦åŒ…å«å®ä½“å ä½ç¬¦
            entity_pattern = re.compile(r'\[([^\]]+)\]')
            matches = entity_pattern.findall(utterance_text)
            
            if matches:
                # æœ‰å®ä½“å ä½ç¬¦ï¼Œéœ€è¦å±•å¼€
                for entity_type in matches:
                    if entity_type in slot_mapping:
                        used_entities.add(entity_type)
                        # å±•å¼€ä¸ºå…·ä½“çš„è®­ç»ƒæ ·ä¾‹
                        expanded_examples = expand_entity_placeholder(
                            utterance_text, entity_type, slot_mapping, max_examples=3
                        )
                        fixed_utterances.extend(expanded_examples)
                        break  # åªå¤„ç†ç¬¬ä¸€ä¸ªå ä½ç¬¦
                else:
                    # æ²¡æœ‰æ‰¾åˆ°åŒ¹é…çš„å®ä½“ç±»å‹ï¼Œç§»é™¤æ–¹æ‹¬å·
                    clean_text = re.sub(r'\[([^\]]+)\]', r'\1', utterance_text)
                    fixed_utterances.append({
                        "text": clean_text,
                        "entities": []
                    })
            else:
                # æ²¡æœ‰å®ä½“å ä½ç¬¦ï¼Œç›´æ¥æ·»åŠ 
                fixed_utterances.append({
                    "text": utterance_text,
                    "entities": []
                })
        
        total_fixed_utterances += len(fixed_utterances)
        
        # æ„å»ºä¿®å¤åçš„æ„å›¾
        fixed_intent = {
            "intent_name": intent_name,
            "description": intent.get('description', ''),
            "utterances": fixed_utterances,
            "responses": intent.get('responses', []),
            "entities": list(used_entities),
            "slots": list(used_entities)
        }
        
        fixed_intents.append(fixed_intent)
        
        print(f"  {intent_name}: {len(original_utterances)} â†’ {len(fixed_utterances)} ä¸ªæ ·ä¾‹")
    
    # æ„å»ºå®Œæ•´çš„ä¿®å¤åæ•°æ®
    fixed_data = {
        "intents": fixed_intents,
        "entities": [{"entity": entity_type, "values": slot_mapping[entity_type]} 
                    for entity_type in slot_mapping.keys()],
        "slots": {entity_type: {"type": "categorical", "values": values} 
                 for entity_type, values in slot_mapping.items()},
        "metadata": {
            "total_intents": len(fixed_intents),
            "total_entities": len(slot_mapping),
            "total_slots": len(slot_mapping),
            "total_utterances": total_fixed_utterances,
            "original_utterances": total_original_utterances
        }
    }
    
    # ä¿å­˜ä¿®å¤åçš„æ•°æ®
    output_file = "intents_fixed.json"
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(fixed_data, f, ensure_ascii=False, indent=2)
    
    print(f"\nâœ… ä¿®å¤å®Œæˆï¼")
    print(f"   åŸå§‹è®­ç»ƒæ ·ä¾‹: {total_original_utterances}")
    print(f"   ä¿®å¤åæ ·ä¾‹: {total_fixed_utterances}")
    print(f"   è¾“å‡ºæ–‡ä»¶: {output_file}")
    
    return fixed_data

def generate_fixed_rasa_data():
    """åŸºäºä¿®å¤åçš„æ•°æ®ç”ŸæˆRasaè®­ç»ƒæ–‡ä»¶"""
    
    # åŠ è½½ä¿®å¤åçš„æ•°æ®
    try:
        with open("intents_fixed.json", 'r', encoding='utf-8') as f:
            data = json.load(f)
    except Exception as e:
        print(f"âŒ æ— æ³•åŠ è½½ä¿®å¤åçš„æ•°æ®: {e}")
        return
    
    print(f"\nç”Ÿæˆä¿®å¤åçš„Rasaè®­ç»ƒæ–‡ä»¶...")
    
    # ç”ŸæˆNLUæ•°æ®
    nlu_data = {
        'version': '3.1',
        'nlu': []
    }
    
    for intent in data['intents']:
        intent_name = intent['intent_name']
        
        examples = []
        for utterance in intent['utterances']:
            if utterance['entities']:
                # æœ‰å®ä½“çš„æ ·ä¾‹
                text = utterance['text']
                entities_text = ""
                
                sorted_entities = sorted(utterance['entities'], key=lambda x: x['start'])
                
                current_pos = 0
                for entity in sorted_entities:
                    # æ·»åŠ å®ä½“å‰çš„æ–‡æœ¬
                    if entity['start'] > current_pos:
                        entities_text += text[current_pos:entity['start']]
                    
                    # æ·»åŠ å®ä½“æ ‡æ³¨
                    entity_text = text[entity['start']:entity['end']]
                    entities_text += f"[{entity_text}]({entity['entity']})"
                    
                    current_pos = entity['end']
                
                # æ·»åŠ å‰©ä½™æ–‡æœ¬
                if current_pos < len(text):
                    entities_text += text[current_pos:]
                
                examples.append(f"- {entities_text}")
            else:
                # æ²¡æœ‰å®ä½“çš„æ ·ä¾‹
                examples.append(f"- {utterance['text']}")
        
        if examples:
            nlu_data['nlu'].append({
                'intent': intent_name,
                'examples': '\n'.join(examples)
            })
    
    # ç”ŸæˆDomainæ•°æ®
    domain_data = {
        'version': '3.1',
        'intents': [intent['intent_name'] for intent in data['intents']],
        'entities': list(data['slots'].keys()),
        'slots': {},
        'responses': {}
    }
    
    # æ·»åŠ æ§½ä½å®šä¹‰
    for slot_name, slot_info in data['slots'].items():
        domain_data['slots'][slot_name] = {
            'type': 'categorical',
            'values': slot_info['values'],
            'mappings': [
                {
                    'type': 'from_entity',
                    'entity': slot_name
                }
            ]
        }
    
    # æ·»åŠ å“åº”
    for intent in data['intents']:
        intent_name = intent['intent_name']
        response_key = f"utter_{intent_name}"
        
        responses = []
        for response in intent['responses']:
            if response['type'] == 'success':
                responses.append({'text': response['text']})
        
        if not responses:
            responses.append({'text': f"å·²æ‰§è¡Œ{intent.get('description', intent_name)}"})
        
        domain_data['responses'][response_key] = responses
    
    # ä¿å­˜æ–‡ä»¶
    output_dir = "../rasa/data"
    os.makedirs(output_dir, exist_ok=True)
    
    # ä¿å­˜NLUæ–‡ä»¶
    with open(os.path.join(output_dir, "nlu_fixed.yml"), 'w', encoding='utf-8') as f:
        yaml.dump(nlu_data, f, default_flow_style=False, allow_unicode=True, sort_keys=False)
    
    # ä¿å­˜Domainæ–‡ä»¶
    with open(os.path.join(output_dir, "domain_fixed.yml"), 'w', encoding='utf-8') as f:
        yaml.dump(domain_data, f, default_flow_style=False, allow_unicode=True, sort_keys=False)
    
    print(f"âœ… ç”Ÿæˆä¿®å¤åçš„Rasaæ–‡ä»¶:")
    print(f"   - {os.path.join(output_dir, 'nlu_fixed.yml')}")
    print(f"   - {os.path.join(output_dir, 'domain_fixed.yml')}")
    
    # æ˜¾ç¤ºä¿®å¤æ•ˆæœç¤ºä¾‹
    print(f"\nğŸ“‹ ä¿®å¤æ•ˆæœç¤ºä¾‹:")
    for intent in data['intents'][:3]:
        if intent['entities']:
            print(f"  æ„å›¾: {intent['intent_name']}")
            for utterance in intent['utterances'][:2]:
                if utterance['entities']:
                    entity_info = utterance['entities'][0]
                    print(f"    âœ… {utterance['text']} -> [{entity_info['value']}]({entity_info['entity']})")

if __name__ == "__main__":
    # ä¿®å¤è®­ç»ƒæ•°æ®
    fix_training_data()
    
    # ç”Ÿæˆä¿®å¤åçš„Rasaè®­ç»ƒæ–‡ä»¶
    generate_fixed_rasa_data()
 